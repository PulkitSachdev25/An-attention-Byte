# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rrFW07c-IGmeD-P4fHymfXLno3S40G3-
"""

# =========================
# INSTALLS
# =========================
!apt-get update -qq
!apt-get install -y ffmpeg
!pip install gradio moviepy ffmpeg-python pydub faster-whisper vaderSentiment opencv-python-headless

# =========================
# GRADIO APP
# =========================
import gradio as gr
import os, cv2, ffmpeg, numpy as np
from moviepy.editor import VideoFileClip, VideoClip, CompositeVideoClip, ColorClip
from pydub import AudioSegment
from pydub.utils import make_chunks
from faster_whisper import WhisperModel
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def process_video(video_file):
    try:
        os.makedirs("temp_output", exist_ok=True)
        import shutil
        shutil.copy(video_file, "input.mp4")
        return "Saved video. Now processingâ€¦", []
    except Exception as e:
        return f"Error saving video: {e}", []
# ---------------- GRADIO INTERFACE ----------------
iface = gr.Interface(
    fn=process_video,
    inputs=gr.Video(label="Upload MP4"),
    outputs=[gr.Textbox(label="Status"), gr.File(label="Download Reels")],
    title="ðŸŽ¬ Reel-Worthy Clip Generator"
)

iface.launch(share=True)

from moviepy.editor import VideoFileClip
!pip install textblob
!python -m textblob.download_corpora

!apt-get update -qq
!apt-get install -y ffmpeg
!pip install ffmpeg-python

video_path = "test.mp4"
clip = VideoFileClip(video_path)
duration = clip.duration        # seconds (float)
fps = clip.fps
width, height = clip.size

print(f"Duration: {duration:.2f} seconds")
print(f"FPS: {fps}")
print(f"Resolution: {width}x{height}")

import ffmpeg
audio_path = "myaudio.wav"
ffmpeg.input(video_path).output(audio_path).run(overwrite_output=True)
print("audio extracted to:",audio_path)

from pydub import AudioSegment
from pydub.utils import make_chunks

audio = AudioSegment.from_wav("myaudio.wav")

# Desired number of candidate clips
CLIP_MIN_PEAKS = 10
CLIP_MAX_PEAKS = 15
CHUNK_MS = 1000  # 1 second chunks
MERGE_GAP = 5000  # merge peaks within 5 seconds
TARGET_CLIP_DURATION = 60  # seconds

THRESHOLD_START = -15
THRESHOLD_MIN = -40
THRESHOLD_STEP = 1

threshold_db = THRESHOLD_START
success = False

while threshold_db >= THRESHOLD_MIN:
    chunks = make_chunks(audio, CHUNK_MS)
    peaks = []

    # Step 1: find peaks above threshold
    for i, chunk in enumerate(chunks):
        if chunk.dBFS >= threshold_db:
            peaks.append(i * CHUNK_MS)

    # Step 2: merge nearby peaks
    merged = []
    if peaks:
        start = peaks[0]
        end = peaks[0]
        for t in peaks[1:]:
            if t - end <= MERGE_GAP:
                end = t
            else:
                merged.append([start / 1000, (end + CHUNK_MS) / 1000])
                start = end = t
        merged.append([start / 1000, (end + CHUNK_MS) / 1000])

        # Step 3: expand/clip to ~60 seconds
        final_clips = []
        for s, e in merged:
            duration = e - s
            if duration < TARGET_CLIP_DURATION:
                # try to expand equally on both sides without exceeding audio limits
                extra = TARGET_CLIP_DURATION - duration
                new_start = max(0, s - extra / 2)
                new_end = min(audio.duration_seconds, e + extra / 2)
                final_clips.append([new_start, new_end])
            elif duration > TARGET_CLIP_DURATION:
                # if too long, just take the first 60 seconds
                final_clips.append([s, s + TARGET_CLIP_DURATION])
            else:
                final_clips.append([s, e])
        merged = final_clips

    else:
        merged = []

    print(f"Threshold: {threshold_db} dB | Candidate clips: {len(merged)}")

    if CLIP_MIN_PEAKS <= len(merged) <= CLIP_MAX_PEAKS:
        success = True
        break
    elif len(merged) < CLIP_MIN_PEAKS:
        threshold_db -= THRESHOLD_STEP
    else:
        threshold_db += THRESHOLD_STEP

if not success:
    print("Couldn't find proper peaks for desired clip count!")
else:
    print("Final candidate clips (seconds):", merged)

!pip install faster-whisper

from pydub import AudioSegment
from faster_whisper import WhisperModel

audio = AudioSegment.from_file("myaudio.wav")
model = WhisperModel("medium", device="cuda", compute_type="int8")

all_clip_transcripts = []  # store transcripts per clip

for i, (start, end) in enumerate(merged, 1):
    start_ms, end_ms = int(start * 1000), int(end * 1000)
    clip_audio = audio[start_ms:end_ms]

    temp_path = f"audio_clip_{i}.wav"
    clip_audio.export(temp_path, format="wav")

    segments, info = model.transcribe(temp_path, beam_size=5, language="en")

    clip_transcript = []
    for seg in segments:
        clip_transcript.append({
            "start": seg.start,  # relative to this audio clip
            "end": seg.end,
            "text": seg.text.strip()
        })

    # store transcript under clip index
    all_clip_transcripts.append({
        "clip_idx": i,
        "transcript": clip_transcript
    })

# Example: printing per clip
for clip in all_clip_transcripts:
    print(f"Clip {clip['clip_idx']} transcript:")
    for seg in clip['transcript']:
        print(f"{seg['start']:.2f}-{seg['end']:.2f}: {seg['text']}")
    print("---")

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

# Store clip sentiment and priority
clip_sentiments = []

for clip in all_clip_transcripts:
    clip_idx = clip['clip_idx']

    # Merge all text segments into one string per clip
    full_text = " ".join([seg['text'] for seg in clip['transcript']])

    # Compute VADER sentiment
    score = analyzer.polarity_scores(full_text)['compound']  # compound score ranges -1 to 1

       # Get clip start and end times from segments
    clip_start = clip['transcript'][0]['start']
    clip_end = clip['transcript'][-1]['end']
    duration = clip_end - clip_start
    clip_sentiments.append({
        "clip_idx": clip_idx,
        "score": score,
        "text": full_text
    })

# Sort clips by sentiment descending (best first)
clip_sentiments.sort(key=lambda x: x['score'], reverse=True)

# Take top 5 clips (or fewer if less exist)
top_clips = clip_sentiments[:5]

# Assign priority (1 = best)
for priority, clip in enumerate(top_clips, 1):
    clip['priority'] = priority

# Print final selection
print("Top clips by sentiment priority:")
for clip in top_clips:
    print(f"Priority {clip['priority']}: Clip {clip['clip_idx']} | Score={clip['score']:.2f}")
    print(f"Text (truncated): {clip['text'][:150]}...")
    print("---")

import os
import cv2
import numpy as np
from moviepy.editor import VideoFileClip, VideoClip, CompositeVideoClip, ColorClip, TextClip

# ----------------------------
# 1. Add start/end/duration to top clips
# ----------------------------
for clip in top_clips:
    idx = clip["clip_idx"] - 1  # zero-based index
    clip["start"] = merged[idx][0]
    clip["end"]   = merged[idx][1]
    clip["duration"] = clip["end"] - clip["start"]

# ----------------------------
# 2. Video settings
# ----------------------------
TARGET_WIDTH = 1080
TARGET_HEIGHT = 1920
TARGET_ASPECT = TARGET_WIDTH / TARGET_HEIGHT

FONT_SCALE = 1.2
THICKNESS = 3
PADDING = 30  # distance from bottom
LINE_SPACING = 10
FONT = cv2.FONT_HERSHEY_SIMPLEX
TEXT_COLOR = (255, 255, 255)
BG_COLOR = (0, 0, 0)
MAX_WIDTH_RATIO = 0.9

# Ensure output directories exist
os.makedirs("final_clips", exist_ok=True)
os.makedirs("final_clips_vertical", exist_ok=True)
os.makedirs("final_clips_subtitled", exist_ok=True)

# Load full video
full_video_path = "test.mp4"
full_video = VideoFileClip(full_video_path)

# ----------------------------
# 3. Process each top clip
# ----------------------------
for priority, clip in enumerate(top_clips, 1):
    start_time = clip["start"]
    end_time   = clip["end"]

    # 3a. Extract subclip
    subclip = full_video.subclip(start_time, end_time)
    temp_clip_path = f"final_clips/clip_{priority}_idx{clip['clip_idx']}.mp4"
    subclip.write_videofile(temp_clip_path, codec="libx264", audio_codec="aac", fps=24)
    print(f"Saved Clip {priority} | Duration: {clip['duration']:.2f}s")

    # 3b. Convert to vertical 9:16
    w, h = subclip.size
    aspect = w / h

    if aspect > TARGET_ASPECT:
        # Wider than 9:16
        new_w = TARGET_WIDTH
        new_h = int(TARGET_WIDTH / aspect)
    else:
        # Taller than 9:16
        new_h = TARGET_HEIGHT
        new_w = int(TARGET_HEIGHT * aspect)

    video_resized = subclip.resize(newsize=(new_w, new_h))
    background = ColorClip(size=(TARGET_WIDTH, TARGET_HEIGHT), color=(0,0,0)).set_duration(video_resized.duration)
    vertical_clip = CompositeVideoClip([background.set_position("center"), video_resized.set_position("center")])
    vertical_path = f"final_clips_vertical/clip_{priority}_idx{clip['clip_idx']}_vertical.mp4"
    vertical_clip.write_videofile(vertical_path, fps=24, audio_codec="aac")
    print(f"Saved vertical Clip {priority}")

    # 3c. Add subtitles
    # Get transcript for this clip
    clip_transcript = all_clip_transcripts[clip["clip_idx"] - 1]["transcript"]
    segments = []
    for seg in clip_transcript:
        # clip-relative timestamps
        rel_start = max(0, seg['start'])
        rel_end   = min(subclip.duration, seg['end'])
        segments.append({
            "start": rel_start,
            "end": rel_end,
            "text": seg['text']
        })

    # Function to draw subtitles on each frame
    def make_frame(t):
        frame = vertical_clip.get_frame(t)
        frame = np.array(frame)
        active_subs = [s for s in segments if s['start'] <= t <= s['end']]
        if not active_subs:
            return frame

        for sub in active_subs:
            text = sub['text']
            max_width = int(vertical_clip.w * MAX_WIDTH_RATIO)
            # Wrap text
            words = text.split()
            lines = []
            current_line = ""
            for word in words:
                test_line = current_line + " " + word if current_line else word
                (tw, th), _ = cv2.getTextSize(test_line, FONT, FONT_SCALE, THICKNESS)
                if tw > max_width:
                    lines.append(current_line)
                    current_line = word
                else:
                    current_line = test_line
            if current_line:
                lines.append(current_line)

            # Draw background rectangle
            total_text_height = len(lines) * (int(FONT_SCALE*30) + LINE_SPACING)
            y0 = vertical_clip.h - total_text_height - PADDING
            y1 = y0 + total_text_height + 20
            cv2.rectangle(frame, (0, y0-10), (vertical_clip.w, y1), BG_COLOR, -1)

            # Draw each line
            y = y0
            for line in lines:
                (tw, th), _ = cv2.getTextSize(line, FONT, FONT_SCALE, THICKNESS)
                x = int((vertical_clip.w - tw)/2)
                cv2.putText(frame, line, (x, y+th), FONT, FONT_SCALE, TEXT_COLOR, THICKNESS, cv2.LINE_AA)
                y += th + LINE_SPACING
        return frame

    subtitled_clip = VideoClip(make_frame, duration=vertical_clip.duration)
    subtitled_clip = subtitled_clip.set_audio(vertical_clip.audio)
    final_sub_path = f"final_clips_subtitled/clip_{priority}_idx{clip['clip_idx']}_subtitled.mp4"
    subtitled_clip.write_videofile(final_sub_path, fps=24, audio_codec="aac")
    print(f"Saved subtitled Clip {priority}")